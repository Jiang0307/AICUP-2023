{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7MSEcenjVrL"
   },
   "source": [
    "## PART 1. Document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "niqu9pLajYC_"
   },
   "outputs": [],
   "source": [
    "# original import\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "# 3rd party libs\n",
    "import hanlp\n",
    "import opencc\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from hanlp.components.pipeline import Pipeline\n",
    "from pandarallel import pandarallel\n",
    "# our own libs\n",
    "from utils import load_json, generate_evidence_to_wiki_pages_mapping\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
    "wikipedia.set_lang(\"zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW3 import\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import scipy\n",
    "\n",
    "jieba.set_dictionary(\"dict.txt.big\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "# Adjust the number of workers if you want\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from hw3_utils import (jsonl_dir_to_df, calculate_precision, calculate_recall)\n",
    "from TCSP import read_stopwords_list\n",
    "stopwords = read_stopwords_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload the data.\n",
    "\n",
    "# TRAIN_DATA = load_json(\"data/public_train_0522.jsonl\")\n",
    "# TEST_DATA = load_json(\"data/private_test_data.jsonl\")\n",
    "# CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
    "# CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data class for type hinting\n",
    "\n",
    "@dataclass\n",
    "class Claim:\n",
    "    data: str\n",
    "@dataclass\n",
    "class AnnotationID:\n",
    "    id: int\n",
    "@dataclass\n",
    "class EvidenceID:\n",
    "    id: int\n",
    "@dataclass\n",
    "class PageTitle:\n",
    "    title: str\n",
    "@dataclass\n",
    "class SentenceID:\n",
    "    id: int\n",
    "@dataclass\n",
    "class Evidence:\n",
    "    data: List[List[Tuple[AnnotationID, EvidenceID, PageTitle, SentenceID]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str, stopwords: list) -> str:\n",
    "    # 用jieba把整段句子拆成token list\n",
    "    tokens = list(jieba.cut(text))\n",
    "    result = \" \".join([word for word in tokens if word not in stopwords])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default amount of documents retrieved is at most five documents.  This `num_pred_doc` can be adjusted based on your objective.  Save data in jsonl format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    "    mode: str = \"train\",\n",
    "    num_pred_doc: int = 5,\n",
    ") -> None:\n",
    "    with open(\n",
    "        f\"data/{mode}_doc{num_pred_doc}.jsonl\",\n",
    "        \"w\",\n",
    "        encoding=\"utf8\",\n",
    "    ) as f:\n",
    "        for i, d in enumerate(data):\n",
    "            d[\"predicted_pages\"] = list(predictions.iloc[i])\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_docs_sklearn_sentence(\n",
    "    claim: str,\n",
    "    tokenizing_method: callable,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    topk: int,\n",
    ") -> set:\n",
    "    global wiki_sentences\n",
    "    tokens = tokenizing_method(claim)\n",
    "    claim_vector = vectorizer.transform([tokens])\n",
    "    # TODO: Write your code here\n",
    "    similarity_scores = cosine_similarity(claim_vector , X)\n",
    "    # flatten the array\n",
    "    similarity_scores = similarity_scores[0, :]\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    # TODO: Write your code here\n",
    "    sorted_indices = similarity_scores.argsort()[::-1]\n",
    "    topk_sorted_indices = sorted_indices[:1000]\n",
    "    results = []\n",
    "    for idx in topk_sorted_indices:\n",
    "        real_id = wiki_sentences.iloc[idx]['id']\n",
    "        if(real_id not in results):\n",
    "            results.append(real_id)\n",
    "            if(len(results) == topk):\n",
    "                break\n",
    "            \n",
    "    exact_matchs = []\n",
    "    Count = 0\n",
    "    for i,result in enumerate(results):\n",
    "        if (\n",
    "            (result in claim)\n",
    "            or (result in claim.replace(\" \", \"\"))\n",
    "            or (result.replace(\"·\", \"\") in claim)\n",
    "            or (result.replace(\"-\", \"\") in claim)\n",
    "        ):\n",
    "            exact_matchs.append(result)\n",
    "        elif \"·\" in result:\n",
    "            splitted = result.split(\"·\")\n",
    "            for split in splitted:\n",
    "                if split in claim:\n",
    "                    exact_matchs.append(result)\n",
    "                    break\n",
    "        elif \"_(\" in result:\n",
    "            splitted = result.split(\"_(\")\n",
    "            splitted[1] = splitted[1][:-1]\n",
    "            \n",
    "            for split in splitted:\n",
    "                if(\")\" in split):\n",
    "                    split = split[:-1]\n",
    "                    \n",
    "                if split in claim:\n",
    "                    exact_matchs.append(result)\n",
    "                    break\n",
    "            \n",
    "    return set(exact_matchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First time running this cell will 34 minutes using Google Colab.\n",
    "wiki_path = \"data/wiki-pages\"\n",
    "wiki_cache = \"wiki\"\n",
    "target_column = \"text\"\n",
    "\n",
    "wiki_cache_path = Path(f\"data/{wiki_cache}.pkl\")\n",
    "if wiki_cache_path.exists():\n",
    "    wiki_pages = pd.read_pickle(wiki_cache_path)\n",
    "else:\n",
    "    wiki_pages = jsonl_dir_to_df(wiki_path)\n",
    "    wiki_pages = wiki_pages.reset_index(drop=True)\n",
    "    wiki_pages[\"processed_text\"] = wiki_pages[target_column].progress_apply(\n",
    "        partial(tokenize, stopwords=stopwords)\n",
    "    )\n",
    "    # save the result to a pickle file\n",
    "    wiki_pages.to_pickle(wiki_cache_path, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_path = Path(f\"data/mapping_document_retrieval.json\")\n",
    "if mapping_path.exists():\n",
    "    mapping = json.load( open( \"data/mapping_document_retrieval.json\" ) )\n",
    "else:\n",
    "    mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "    json.dump( mapping, open( \"data/mapping_document_retrieval.json\", 'w' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 只取大於min_wiki_length的wiki_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781253"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 參數\n",
    "min_wiki_length = 100\n",
    "min_sentence_length = 15\n",
    "num_of_samples = 500\n",
    "topk = 10\n",
    "use_idf = True\n",
    "sublinear_tf = True\n",
    "\n",
    "wiki_pages = wiki_pages[wiki_pages['processed_text'].str.len() > min_wiki_length]\n",
    "len(wiki_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 產生wiki_sentences.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"data/wiki_sentence.pkl\")\n",
    "if path.exists():\n",
    "    wiki_sentences = pd.read_pickle(\"data/wiki_sentence.pkl\")  \n",
    "else:\n",
    "    data = {'id': [], 'idx': [], 'text': []}\n",
    "    wiki_sentences = []\n",
    "    for i in tqdm(range(len(wiki_pages))):\n",
    "        id = wiki_pages.iloc[i]['id']\n",
    "        for sentence in mapping[id].values():\n",
    "            if(sentence != ''):\n",
    "                dic = {'id':id, 'idx':int(i), 'text':sentence}\n",
    "                wiki_sentences.append(dic)\n",
    "    wiki_sentences = pd.DataFrame(wiki_sentences)\n",
    "    del wiki_pages\n",
    "    wiki_sentences[\"processed_text\"] = wiki_sentences['text'].progress_apply(\n",
    "        partial(tokenize, stopwords=stopwords)\n",
    "    )\n",
    "    wiki_sentences.to_pickle('data/wiki_sentence.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3732467"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF(HW3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從wiki_sentences中取長度大於min_sentence_length的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentences = wiki_sentences[ wiki_sentences['processed_text'].str.len() >= min_sentence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 125 ms\n",
      "Wall time: 124 ms\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
    "doc_path = f\"data/train_doc5.jsonl\"\n",
    "TRAIN_GT = pd.DataFrame(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將public_train.json的資料放入wiki_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from D:\\FDA\\AICUP2023-baseline\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.u9fc27dfe134194f973eec0c33d27bf6b.cache\n",
      "Loading model cost 2.222 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "for i,row in TRAIN_GT.iterrows():\n",
    "    if(row['label'] == 'NOT ENOUGH INFO'):\n",
    "        continue\n",
    "    wiki_names = []\n",
    "    evidence_sets = row['evidence']\n",
    "    for sets in evidence_sets:\n",
    "        for one_set in sets:\n",
    "            if(one_set[2] not in wiki_names):\n",
    "                wiki_names.append(one_set[2])\n",
    "    \n",
    "    claim = tokenize(row['claim']  , stopwords)\n",
    "    if len(wiki_names) > 0:\n",
    "        for name in wiki_names:\n",
    "            dic = {'id':name, 'idx':int(i), 'text':row['claim'], 'processed_text':claim}\n",
    "            train_list.append(dic)\n",
    "        \n",
    "    # 整理成dictionary並新增到wiki_sentences中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>數學</td>\n",
       "      <td>數學 ， 是研究數量 、 結構以及空間等概念及其變化的一門學科 ， 屬於形式科學的一種 。</td>\n",
       "      <td>數學     研究 數量     結構 空間 概念 變化 一門 學科     屬於 形式 科學</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>數學</td>\n",
       "      <td>基礎數學的知識與運用是生活中不可或缺的一環 。</td>\n",
       "      <td>基礎 數學 知識 運用 生活 不可或缺 一環</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>數學</td>\n",
       "      <td>對數學基本概念的完善 ， 早在古埃及 、 美索不達米亞及古印度歷史上的古代數學文本便可觀見 ...</td>\n",
       "      <td>數學 基本概念 完善     早 古埃及     美索不達米亞 古印度 歷史 古代 數學 文...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>數學</td>\n",
       "      <td>從那時開始 ， 數學的發展便持續不斷地小幅進展 ， 至16世紀的文藝復興時期 ， 因爲新的科...</td>\n",
       "      <td>數學 發展 便 持續 不斷 小幅 進展     16 世紀 文藝復興 時期     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>數學</td>\n",
       "      <td>數學併成爲許多國家及地區的教育中的一部分 。</td>\n",
       "      <td>數學 併 成爲 國家 地區 教育 一部分</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398032</th>\n",
       "      <td>京畿道</td>\n",
       "      <td>亞洲有超過一半的人口在京畿道。</td>\n",
       "      <td>亞洲 超過 一半 人口 京畿道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398033</th>\n",
       "      <td>亞洲</td>\n",
       "      <td>亞洲有超過一半的人口在京畿道。</td>\n",
       "      <td>亞洲 超過 一半 人口 京畿道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398034</th>\n",
       "      <td>軟件測試</td>\n",
       "      <td>一種用來抵銷鑑定軟體的過程稱作軟件測試，測試的定義之一是爲了評估而質疑產品的過程。</td>\n",
       "      <td>抵銷 鑑定 軟體 過程 稱作 軟件測試 測試 定義 爲了 評估 質疑 產品 過程</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398035</th>\n",
       "      <td>福山城_(備後國)</td>\n",
       "      <td>備後國的福山城現今位於日本廣島縣，曾在戰爭中遭到攻擊。</td>\n",
       "      <td>備後國 福 山城 現今 位於 日本 廣島 縣 戰爭 遭到 攻擊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398036</th>\n",
       "      <td>蘋果酒</td>\n",
       "      <td>世界上僅次葡萄酒的品牌爲蘋果酒。</td>\n",
       "      <td>世界 上僅次 葡萄酒 品牌 爲 蘋果酒</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3398037 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                               text  \\\n",
       "0               數學      數學 ， 是研究數量 、 結構以及空間等概念及其變化的一門學科 ， 屬於形式科學的一種 。   \n",
       "1               數學                            基礎數學的知識與運用是生活中不可或缺的一環 。   \n",
       "2               數學  對數學基本概念的完善 ， 早在古埃及 、 美索不達米亞及古印度歷史上的古代數學文本便可觀見 ...   \n",
       "3               數學  從那時開始 ， 數學的發展便持續不斷地小幅進展 ， 至16世紀的文藝復興時期 ， 因爲新的科...   \n",
       "4               數學                             數學併成爲許多國家及地區的教育中的一部分 。   \n",
       "...            ...                                                ...   \n",
       "3398032        京畿道                                    亞洲有超過一半的人口在京畿道。   \n",
       "3398033         亞洲                                    亞洲有超過一半的人口在京畿道。   \n",
       "3398034       軟件測試          一種用來抵銷鑑定軟體的過程稱作軟件測試，測試的定義之一是爲了評估而質疑產品的過程。   \n",
       "3398035  福山城_(備後國)                        備後國的福山城現今位於日本廣島縣，曾在戰爭中遭到攻擊。   \n",
       "3398036        蘋果酒                                   世界上僅次葡萄酒的品牌爲蘋果酒。   \n",
       "\n",
       "                                            processed_text  \n",
       "0        數學     研究 數量     結構 空間 概念 變化 一門 學科     屬於 形式 科學    \n",
       "1                                 基礎 數學 知識 運用 生活 不可或缺 一環    \n",
       "2        數學 基本概念 完善     早 古埃及     美索不達米亞 古印度 歷史 古代 數學 文...  \n",
       "3            數學 發展 便 持續 不斷 小幅 進展     16 世紀 文藝復興 時期     ...  \n",
       "4                                   數學 併 成爲 國家 地區 教育 一部分    \n",
       "...                                                    ...  \n",
       "3398032                                    亞洲 超過 一半 人口 京畿道  \n",
       "3398033                                    亞洲 超過 一半 人口 京畿道  \n",
       "3398034           抵銷 鑑定 軟體 過程 稱作 軟件測試 測試 定義 爲了 評估 質疑 產品 過程  \n",
       "3398035                    備後國 福 山城 現今 位於 日本 廣島 縣 戰爭 遭到 攻擊  \n",
       "3398036                                世界 上僅次 葡萄酒 品牌 爲 蘋果酒  \n",
       "\n",
       "[3398037 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sentences = pd.concat([wiki_sentences , pd.DataFrame(train_list)])\n",
    "wiki_sentences = wiki_sentences.drop(['idx'], axis=1)\n",
    "wiki_sentences = wiki_sentences.reset_index(drop=True)\n",
    "wiki_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = wiki_sentences['processed_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the TfidfVectorizer\n",
    "# TODO: Write your code here\n",
    "vectorizer = TfidfVectorizer(use_idf=use_idf, sublinear_tf=sublinear_tf, stop_words = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3969/3969 [4:58:21<00:00,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4h 57min 57s\n",
      "Wall time: 4h 58min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = load_json(\"data/public_train.jsonl\")\n",
    "\n",
    "# encode the corpus with TF-IDF\n",
    "train_df = pd.DataFrame(train)\n",
    "\n",
    "# prediction\n",
    "train_df[\"predicted_pages\"] = train_df[\"claim\"].progress_apply(\n",
    "    partial(\n",
    "        get_pred_docs_sklearn_sentence,\n",
    "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
    "        vectorizer=vectorizer,\n",
    "        topk=topk,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7159799107142859\n",
      "Recall: 0.8962968750000002\n",
      "{'topk': 10, 'num_of_samples': 500, 'min_sentence_length': 15, 'precision': 0.7159799107142859, 'recall': 0.8962968750000002}\n"
     ]
    }
   ],
   "source": [
    "precision = calculate_precision(train, train_df[\"predicted_pages\"])\n",
    "recall = calculate_recall(train, train_df[\"predicted_pages\"])\n",
    "dictionary = {'topk':topk,\n",
    "              'num_of_samples':num_of_samples,\n",
    "              'min_sentence_length':min_sentence_length,\n",
    "              'precision':precision,\n",
    "              'recall':recall}\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_doc(train, train_df[\"predicted_pages\"], mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9038/9038 [11:17:17<00:00,  4.50s/it]\n"
     ]
    }
   ],
   "source": [
    "test = load_json(\"data/all_test_data.jsonl\")\n",
    "\n",
    "test_df = pd.DataFrame(test)\n",
    "# prediction\n",
    "test_df[\"predicted_pages\"] = test_df[\"claim\"].progress_apply(\n",
    "    partial(\n",
    "        get_pred_docs_sklearn_sentence,\n",
    "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
    "        vectorizer=vectorizer,\n",
    "        topk=topk,\n",
    "    )\n",
    ")\n",
    "save_doc(test, test_df[\"predicted_pages\"], mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面是修改TOP N版(舊版)的document retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of consistency, we convert traditional to simplified Chinese first before converting it back to traditional Chinese.  This is due to some errors occuring when converting traditional to traditional Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "A3NU01DnjKp-"
   },
   "outputs": [],
   "source": [
    "# def do_st_corrections(text: str) -> str:\n",
    "#     simplified = CONVERTER_T2S.convert(text)\n",
    "\n",
    "#     return CONVERTER_S2T.convert(simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use constituency parsing to separate part of speeches or so called constituent to extract noun phrases.  In the later stages, we will use the noun phrases as the query to search for relevant documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nps_hanlp(\n",
    "#     predictor: Pipeline,\n",
    "#     d: Dict[str, Union[int, Claim, Evidence]],\n",
    "# ) -> List[str]:\n",
    "#     claim = d[\"claim\"]\n",
    "#     tree = predictor(claim)[\"con\"]\n",
    "#     nps = [\n",
    "#         do_st_corrections(\"\".join(subtree.leaves()))\n",
    "#         for subtree in tree.subtrees(lambda t: t.label() == \"NP\")\n",
    "#     ]\n",
    "\n",
    "#     return nps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision refers to how many related documents are retrieved.  Recall refers to how many relevant documents are retrieved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_precision(\n",
    "#     data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "#     predictions: pd.Series,\n",
    "# ) -> None:\n",
    "#     precision = 0\n",
    "#     count = 0\n",
    "\n",
    "#     for i, d in enumerate(data):\n",
    "#         if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "#             continue\n",
    "\n",
    "#         # Extract all ground truth of titles of the wikipedia pages\n",
    "#         # evidence[2] refers to the title of the wikipedia page\n",
    "#         gt_pages = set([\n",
    "#             evidence[2]\n",
    "#             for evidence_set in d[\"evidence\"]\n",
    "#             for evidence in evidence_set\n",
    "#         ])\n",
    "\n",
    "#         predicted_pages = predictions.iloc[i]\n",
    "#         hits = predicted_pages.intersection(gt_pages)\n",
    "#         if len(predicted_pages) != 0:\n",
    "#             precision += len(hits) / len(predicted_pages)\n",
    "\n",
    "#         count += 1\n",
    "\n",
    "#     # Macro precision\n",
    "#     print(f\"Precision: {precision / count}\")\n",
    "\n",
    "\n",
    "# def calculate_recall(\n",
    "#     data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "#     predictions: pd.Series,\n",
    "# ) -> None:\n",
    "#     recall = 0\n",
    "#     count = 0\n",
    "\n",
    "#     for i, d in enumerate(data):\n",
    "#         if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "#             continue\n",
    "\n",
    "#         gt_pages = set([\n",
    "#             evidence[2]\n",
    "#             for evidence_set in d[\"evidence\"]\n",
    "#             for evidence in evidence_set\n",
    "#         ])\n",
    "#         predicted_pages = predictions.iloc[i]\n",
    "#         hits = predicted_pages.intersection(gt_pages)\n",
    "#         recall += len(hits) / len(gt_pages)\n",
    "#         count += 1\n",
    "\n",
    "#     print(f\"Recall: {recall / count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ayGI44qkk_wy"
   },
   "outputs": [],
   "source": [
    "# def get_pred_pages(series_data: pd.Series) -> Set[Dict[int, str]]:\n",
    "#     results = []\n",
    "#     tmp_muji = []\n",
    "#     # wiki_page: its index showned in claim\n",
    "#     mapping = {}\n",
    "#     claim = series_data[\"claim\"]\n",
    "#     nps = series_data[\"hanlp_results\"]\n",
    "#     first_wiki_term = []\n",
    "\n",
    "#     for i, np in enumerate(nps):\n",
    "#         # Simplified Traditional Chinese Correction\n",
    "#         wiki_search_results = [\n",
    "#             do_st_corrections(w) for w in wikipedia.search(np)\n",
    "#         ]\n",
    "\n",
    "#         # Remove the wiki page's description in brackets\n",
    "#         wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
    "#         wiki_df = pd.DataFrame({\n",
    "#             \"wiki_set\": wiki_set,\n",
    "#             \"wiki_results\": wiki_search_results\n",
    "#         })\n",
    "\n",
    "#         # Elements in wiki_set --> index\n",
    "#         # Extracting only the first element is one way to avoid extracting\n",
    "#         # too many of the similar wiki pages\n",
    "#         grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first()\n",
    "#         candidates = grouped_df[\"wiki_results\"].tolist()\n",
    "#         # muji refers to wiki_set\n",
    "#         muji = grouped_df.index.tolist()\n",
    "\n",
    "#         for prefix, term in zip(muji, candidates):\n",
    "#             if prefix not in tmp_muji:\n",
    "#                 matched = False\n",
    "\n",
    "#                 # Take at least one term from the first noun phrase\n",
    "#                 if i == 0:\n",
    "#                     first_wiki_term.append(term)\n",
    "\n",
    "#                 # Walrus operator :=\n",
    "#                 # https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions\n",
    "#                 # Through these filters, we are trying to figure out if the term\n",
    "#                 # is within the claim\n",
    "#                 if (((new_term := term) in claim) or\n",
    "#                     ((new_term := term.replace(\"·\", \"\")) in claim) or\n",
    "#                     ((new_term := term.split(\" \")[0]) in claim) or\n",
    "#                     ((new_term := term.replace(\"-\", \" \")) in claim)):\n",
    "#                     matched = True\n",
    "\n",
    "#                 elif \"·\" in term:\n",
    "#                     splitted = term.split(\"·\")\n",
    "#                     for split in splitted:\n",
    "#                         if (new_term := split) in claim:\n",
    "#                             matched = True\n",
    "#                             break\n",
    "\n",
    "#                 if matched:\n",
    "#                     # post-processing\n",
    "#                     term = term.replace(\" \", \"_\")\n",
    "#                     term = term.replace(\"-\", \"\")\n",
    "#                     results.append(term)\n",
    "#                     mapping[term] = claim.find(new_term)\n",
    "#                     tmp_muji.append(new_term)\n",
    "\n",
    "#     # 5 is a hyperparameter\n",
    "#     if len(results) > 5:\n",
    "#         assert -1 not in mapping.values()\n",
    "#         results = sorted(mapping, key=mapping.get)[:5]\n",
    "#         # results = sorted(mapping, key=mapping.get)[:5]\n",
    "#     elif len(results) >=3 and len(results) <= 5:\n",
    "#         assert -1 not in mapping.values()\n",
    "#         results = sorted(mapping, key=mapping.get)[:3]\n",
    "#     elif len(results) < 1:\n",
    "#         results = first_wiki_term\n",
    "\n",
    "#     return set(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Get noun phrases from hanlp consituency parsing tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup [HanLP](https://github.com/hankcs/HanLP) predictor (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = (hanlp.pipeline().append(\n",
    "#     hanlp.load(\"FINE_ELECTRA_SMALL_ZH\"),\n",
    "#     output_key=\"tok\",\n",
    "# ).append(\n",
    "#     hanlp.load(\"CTB9_CON_ELECTRA_SMALL\"),\n",
    "#     output_key=\"con\",\n",
    "#     input_key=\"tok\",\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will skip this process which for creating parsing tree when demo on class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanlp_file = f\"data/hanlp_con_results.pkl\"\n",
    "# if Path(hanlp_file).exists():\n",
    "#     with open(hanlp_file, \"rb\") as f:\n",
    "#         hanlp_results = pickle.load(f)\n",
    "# else:\n",
    "#     hanlp_results = [get_nps_hanlp(predictor, d) for d in TRAIN_DATA]\n",
    "#     with open(hanlp_file, \"wb\") as f:\n",
    "#         pickle.dump(hanlp_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get pages via wiki online api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_path = f\"data/train_doc5.jsonl\"\n",
    "# if Path(doc_path).exists():\n",
    "#     with open(doc_path, \"r\", encoding=\"utf8\") as f:\n",
    "#         predicted_results = pd.Series([\n",
    "#             set(json.loads(line)[\"predicted_pages\"])\n",
    "#             for line in f\n",
    "#         ])\n",
    "# else:\n",
    "#     train_df = pd.DataFrame(TRAIN_DATA)\n",
    "#     train_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
    "#     predicted_results = train_df.apply(get_pred_pages, axis=1)\n",
    "#     save_doc(TRAIN_DATA, predicted_results, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Calculate our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_precision(TRAIN_DATA, predicted_results)\n",
    "# calculate_recall(TRAIN_DATA, predicted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Repeat the same process on test set\n",
    "Create parsing tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanlp_test_file = f\"data/hanlp_con_test_results.pkl\"\n",
    "# if Path(hanlp_test_file).exists():\n",
    "#     with open(hanlp_file, \"rb\") as f:\n",
    "#         hanlp_results = pickle.load(f)\n",
    "# else:\n",
    "#     hanlp_results = [get_nps_hanlp(predictor, d) for d in TEST_DATA]\n",
    "#     with open(hanlp_file, \"wb\") as f:\n",
    "#         pickle.dump(hanlp_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_doc_path = f\"data/test_doc5.jsonl\"\n",
    "# if Path(test_doc_path).exists():\n",
    "#     with open(test_doc_path, \"r\", encoding=\"utf8\") as f:\n",
    "#         test_results = pd.Series(\n",
    "#             [set(json.loads(line)[\"predicted_pages\"]) for line in f])\n",
    "# else:\n",
    "#     test_df = pd.DataFrame(TEST_DATA)\n",
    "#     test_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
    "#     test_results = test_df.parallel_apply(get_pred_pages, axis=1)\n",
    "#     save_doc(TEST_DATA, test_results, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol4zFkSbjgXF"
   },
   "source": [
    "## PART 2. Sentence retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GlliDsgXjisj"
   },
   "outputs": [],
   "source": [
    "# built-in libs\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "# third-party libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "from dataset import BERTDataset, Dataset\n",
    "\n",
    "# local libs\n",
    "from utils import (generate_evidence_to_wiki_pages_mapping, jsonl_dir_to_df, load_json, load_model, save_checkpoint, set_lr_scheduler)\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "J3BBLE3_hlPi"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
    "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
    "DOC_DATA = load_json(\"data/train_doc5.jsonl\")\n",
    "\n",
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "_y = [LABEL2ID[data[\"label\"]] for data in TRAIN_DATA]\n",
    "# GT means Ground Truth\n",
    "TRAIN_GT, DEV_GT = train_test_split(\n",
    "    DOC_DATA,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    shuffle=True,\n",
    "    stratify=_y,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload wiki database (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating jsonl files in data/wiki-pages\n",
      "Generate parse mapping\n",
      "Transform to id to evidence_map mapping\n"
     ]
    }
   ],
   "source": [
    "mapping_path = Path(\"data/mapping_sentence_retrieval.json\")\n",
    "if mapping_path.exists():\n",
    "    mapping = json.load( open( \"data/mapping_sentence_retrieval.json\" ) )\n",
    "else:\n",
    "    wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "    mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "    json.dump( mapping, open( \"data/mapping_sentence_retrieval.json\", 'w' ) )\n",
    "    del wiki_pages\n",
    "\n",
    "# OLD VERSION\n",
    "# wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "# mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "# wiki_pages\n",
    "# del wiki_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence_macro_precision(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate precision for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of precision)\n",
    "        [2]: retrieved (denominator of precision)\n",
    "    \"\"\"\n",
    "    this_precision = 0.0\n",
    "    this_precision_hits = 0.0\n",
    "\n",
    "    # Return 0, 0 if label is not enough info since not enough info does not\n",
    "    # contain any evidence.\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # e[2] is the page title, e[3] is the sentence index\n",
    "        all_evi = [[e[2], e[3]]\n",
    "                   for eg in instance[\"evidence\"]\n",
    "                   for e in eg\n",
    "                   if e[3] is not None]\n",
    "        claim = instance[\"claim\"]\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for prediction in predicted_evidence:\n",
    "            if prediction in all_evi:\n",
    "                this_precision += 1.0\n",
    "            this_precision_hits += 1.0\n",
    "\n",
    "        return (this_precision /\n",
    "                this_precision_hits) if this_precision_hits > 0 else 1.0, 1.0\n",
    "\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate recall for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence_macro_recall(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate recall for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of recall)\n",
    "        [2]: relevant (denominator of recall)\n",
    "    \"\"\"\n",
    "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # If there's no evidence to predict, return 1\n",
    "        if len(instance[\"evidence\"]) == 0 or all(\n",
    "            [len(eg) == 0 for eg in instance]):\n",
    "            return 1.0, 1.0\n",
    "\n",
    "        claim = instance[\"claim\"]\n",
    "\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for evidence_group in instance[\"evidence\"]:\n",
    "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
    "            if all([item in predicted_evidence for item in evidence]):\n",
    "                # We only want to score complete groups of evidence. Incomplete\n",
    "                # groups are worthless.\n",
    "                return 1.0, 1.0\n",
    "        return 0.0, 1.0\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the scores of sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    probs: np.ndarray,\n",
    "    df_evidences: pd.DataFrame,\n",
    "    ground_truths: pd.DataFrame,\n",
    "    top_n: int = 5,\n",
    "    cal_scores: bool = True,\n",
    "    save_name: str = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Calculate the scores of sentence retrieval\n",
    "\n",
    "    Args:\n",
    "        probs (np.ndarray): probabilities of the candidate retrieved sentences\n",
    "        df_evidences (pd.DataFrame): the candiate evidence sentences paired with claims\n",
    "        ground_truths (pd.DataFrame): the loaded data of dev.jsonl or test.jsonl\n",
    "        top_n (int, optional): the number of the retrieved sentences. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: F1 score, precision, and recall\n",
    "    \"\"\"\n",
    "    df_evidences[\"prob\"] = probs\n",
    "    top_rows = (\n",
    "        df_evidences.groupby(\"claim\").apply(\n",
    "        lambda x: x.nlargest(top_n, \"prob\"))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if cal_scores:\n",
    "        macro_precision = 0\n",
    "        macro_precision_hits = 0\n",
    "        macro_recall = 0\n",
    "        macro_recall_hits = 0\n",
    "\n",
    "        for i, instance in enumerate(ground_truths):\n",
    "            macro_prec = evidence_macro_precision(instance, top_rows)\n",
    "            macro_precision += macro_prec[0]\n",
    "            macro_precision_hits += macro_prec[1]\n",
    "\n",
    "            macro_rec = evidence_macro_recall(instance, top_rows)\n",
    "            macro_recall += macro_rec[0]\n",
    "            macro_recall_hits += macro_rec[1]\n",
    "\n",
    "        pr = (macro_precision /\n",
    "              macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
    "        rec = (macro_recall /\n",
    "               macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
    "        f1 = 2.0 * pr * rec / (pr + rec)\n",
    "\n",
    "    if save_name is not None:\n",
    "        # write doc7_sent5 file\n",
    "        with open(f\"data/{save_name}\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for instance in ground_truths:\n",
    "                claim = instance[\"claim\"]\n",
    "                predicted_evidence = top_rows[\n",
    "                    top_rows[\"claim\"] == claim][\"predicted_evidence\"].tolist()\n",
    "                instance[\"predicted_evidence\"] = predicted_evidence\n",
    "                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if cal_scores:\n",
    "        return {\"F1 score\": f1, \"Precision\": pr, \"Recall\": rec}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference script to get probabilites for the candidate evidence sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_probs(\n",
    "    model: nn.Module,\n",
    "    dataloader: Dataset,\n",
    "    device: torch.device,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Inference script to get probabilites for the candidate evidence sentences\n",
    "\n",
    "    Args:\n",
    "        model: the one from HuggingFace Transformers\n",
    "        dataloader: devset or testset in torch dataloader\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: probabilites of the candidate evidence sentences\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            probs.extend(torch.softmax(logits, dim=1)[:, 1].tolist())\n",
    "\n",
    "    return np.array(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentRetrievalBERTDataset(BERTDataset):\n",
    "    \"\"\"AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        item = self.data.iloc[idx]\n",
    "        sentA = item[\"claim\"]\n",
    "        sentB = item[\"text\"]\n",
    "\n",
    "        # claim [SEP] text\n",
    "        concat = self.tokenizer(\n",
    "            sentA,\n",
    "            sentB,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
    "        if \"label\" in item:\n",
    "            concat_ten[\"labels\"] = torch.tensor(item[\"label\"])\n",
    "\n",
    "        return concat_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "gpvXpFwXszfv"
   },
   "outputs": [],
   "source": [
    "def pair_with_wiki_sentences(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    negative_ratio: float,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating train sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # positive\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "        evidence_sets = df[\"evidence\"].iloc[i]\n",
    "        for evidence_set in evidence_sets:\n",
    "            sents = []\n",
    "            for evidence in evidence_set:\n",
    "                # evidence[2] is the page title\n",
    "                page = evidence[2].replace(\" \", \"_\")\n",
    "                # the only page with weird name\n",
    "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                    continue\n",
    "                # evidence[3] is in form of int however, mapping requires str\n",
    "                sent_idx = str(evidence[3])\n",
    "                sents.append(mapping[page][sent_idx])\n",
    "\n",
    "            whole_evidence = \" \".join(sents)\n",
    "\n",
    "            claims.append(claim)\n",
    "            sentences.append(whole_evidence)\n",
    "            labels.append(1)\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        evidence_set = set([(evidence[2], evidence[3])\n",
    "                            for evidences in df[\"evidence\"][i]\n",
    "                            for evidence in evidences])\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [\n",
    "                    (page, sent_idx) for sent_idx in mapping[page].keys()\n",
    "                ]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for pair in page_sent_id_pairs:\n",
    "                if pair in evidence_set:\n",
    "                    continue\n",
    "                text = mapping[page][pair[1]]\n",
    "                # `np.random.rand(1) <= 0.05`: Control not to add too many negative samples\n",
    "                if text != \"\" and np.random.rand(1) <= negative_ratio:\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text)\n",
    "                    labels.append(0)\n",
    "\n",
    "    return pd.DataFrame({\"claim\": claims, \"text\": sentences, \"label\": labels})\n",
    "\n",
    "\n",
    "def pair_with_wiki_sentences_eval(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    is_testset: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating dev and test sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    evidence = []\n",
    "    predicted_evidence = []\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        # if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "        #     continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for page_name, sentence_id in page_sent_id_pairs:\n",
    "                text = mapping[page][sentence_id]\n",
    "                if text != \"\":\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text)\n",
    "                    if not is_testset:\n",
    "                        evidence.append(df[\"evidence\"].iloc[i])\n",
    "                    predicted_evidence.append([page_name, int(sentence_id)])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"claim\": claims,\n",
    "        \"text\": sentences,\n",
    "        \"evidence\": evidence if not is_testset else None,\n",
    "        \"predicted_evidence\": predicted_evidence,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Setup training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"}\n",
    "NUM_EPOCHS = 10  #@param {type:\"integer\"}\n",
    "LR = 2e-5  #@param {type:\"number\"}\n",
    "TRAIN_BATCH_SIZE = 8  #@param {type:\"integer\"}\n",
    "TEST_BATCH_SIZE = 8  #@param {type:\"integer\"}\n",
    "MAX_SEQ_LEN =  256  #@param {type:\"integer\"}\n",
    "NEGATIVE_RATIO = 0.1  #@param {type:\"number\"}\n",
    "VALIDATION_STEP = 100  #@param {type:\"integer\"}\n",
    "TOP_N = 5  #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "ToLvE9oxIXQo"
   },
   "outputs": [],
   "source": [
    "EXP_DIR = f\"sent_retrieval/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_neg{NEGATIVE_RATIO}_top{TOP_N}\"\n",
    "LOG_DIR = \"logs/\" + EXP_DIR\n",
    "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
    "\n",
    "if not Path(LOG_DIR).exists():\n",
    "    Path(LOG_DIR).mkdir(parents=True)\n",
    "\n",
    "if not Path(CKPT_DIR).exists():\n",
    "    Path(CKPT_DIR).mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Combine claims and evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "4A5vWEzPiXGF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using the following train data with 0 (Negative) and 1 (Positive)\n",
      "0    4397\n",
      "1    2774\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df = pair_with_wiki_sentences(\n",
    "    mapping,\n",
    "    pd.DataFrame(TRAIN_GT),\n",
    "    NEGATIVE_RATIO,\n",
    ")\n",
    "counts = train_df[\"label\"].value_counts()\n",
    "print(\"Now using the following train data with 0 (Negative) and 1 (Positive)\")\n",
    "print(counts)\n",
    "\n",
    "dev_evidences = pair_with_wiki_sentences_eval(mapping, pd.DataFrame(DEV_GT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "l48WifjeIGui"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = SentRetrievalBERTDataset(train_df, tokenizer=tokenizer, max_length=MAX_SEQ_LEN)\n",
    "val_dataset = SentRetrievalBERTDataset(dev_evidences, tokenizer=tokenizer, max_length=MAX_SEQ_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    ")\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "4rl_u0YbeQtY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
    "\n",
    "writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure that you are using gpu when training (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "1AHGaKh1eKmg"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f770f12b45b444794f6b364d367bea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e4517c6cb24ab79eb1fda3deaf892c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.41048323729701325, 'Precision': 0.28958333333333247, 'Recall': 0.7046875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2173ff23eee042d08243c3501937381d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.45497366193208405, 'Precision': 0.319895833333332, 'Recall': 0.7875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9479e1339593428888be21a1a240dbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4634112906933956, 'Precision': 0.32489583333333183, 'Recall': 0.8078125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da9d5d03095467785516b78b7831119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4665389607432355, 'Precision': 0.32645833333333174, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ab9dfb2ccc4ef184aaf2b51b633a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.465261124281935, 'Precision': 0.3252083333333317, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19895aa0b6b747a5bb276e1bc8bbb44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4641794120340673, 'Precision': 0.3248958333333317, 'Recall': 0.8125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34480105c53488da232e8522ec1acfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46590039187095444, 'Precision': 0.32583333333333175, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424ccf463f35495d9b8289704e288a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.45965037871789977, 'Precision': 0.3214583333333318, 'Recall': 0.80625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8af4aef320476ca717685a43675be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46347540045766433, 'Precision': 0.32395833333333174, 'Recall': 0.8140625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e12474d18ab4ed2b4ab21706087f793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46672695575140666, 'Precision': 0.3261458333333317, 'Recall': 0.8203125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d3833265774b879afd205d42a176dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4646879852041266, 'Precision': 0.3248958333333317, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6a7a48f8b7442c8256ea56a312a384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4476575150014273, 'Precision': 0.31395833333333195, 'Recall': 0.7796875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e91c739396342cab4ccba3bf5d478a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4656461033035209, 'Precision': 0.3258333333333318, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6eb2b47d854127a93aca91bb6bf31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4646211568287549, 'Precision': 0.3245833333333317, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718b95424e664d99bc5e0bae5e21ad60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4620746735331969, 'Precision': 0.3233333333333318, 'Recall': 0.809375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42169798dee4d29b034f20d04928b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4646211568287549, 'Precision': 0.3245833333333318, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ec9354f5f040cfaf2ebffbc673b0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46915228650262186, 'Precision': 0.32802083333333176, 'Recall': 0.8234375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9acbb26e29a415fb5ff6685bfe4b431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4632865548724521, 'Precision': 0.32427083333333173, 'Recall': 0.8109375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac1992ba0ad4b7198b77fafb70596f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46576623169955284, 'Precision': 0.3252083333333318, 'Recall': 0.8203125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d79a6501b44553b5f4cdc8554bdbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4649412281101772, 'Precision': 0.3248958333333318, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96288d4b6fa4a49a7211645b82417b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46672695575140666, 'Precision': 0.3261458333333317, 'Recall': 0.8203125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a127766c6546aea90cfd293491ddd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46672695575140666, 'Precision': 0.3261458333333317, 'Recall': 0.8203125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49b9bbf416a4d2192e4ef244db3631d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.45543544457977936, 'Precision': 0.31958333333333194, 'Recall': 0.7921875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e932d316844e42b08efc01571b6db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4667932508641061, 'Precision': 0.3264583333333318, 'Recall': 0.81875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87442510faa9476db66922b3b94e32bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.45965037871789993, 'Precision': 0.32145833333333196, 'Recall': 0.80625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e59e74d285441688abae22805270dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.459715126225262, 'Precision': 0.3217708333333319, 'Recall': 0.8046875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c512eec4fd43b29ab9cc0474b92988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4609894582486591, 'Precision': 0.32302083333333187, 'Recall': 0.8046875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1433385185f4d00bb5f4d9122958974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4643682623789496, 'Precision': 0.3245833333333318, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f860ce6a4bd41579f87c771cf424c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.464621156828755, 'Precision': 0.32458333333333184, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c64ce785246410e81cf2b68017e0cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46583409194355785, 'Precision': 0.3255208333333318, 'Recall': 0.81875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d7bba8ab5d4d1b88f60ed8c52ea865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46551402294663835, 'Precision': 0.32520833333333177, 'Recall': 0.81875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524bc2c7578041eeb110cae0b936b16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4646879852041266, 'Precision': 0.3248958333333318, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9f1a209a6b4419b4ec8c4305371144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46150084054900364, 'Precision': 0.32302083333333187, 'Recall': 0.8078125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be73c9c8ff514400952bb29281df02f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46755197481655775, 'Precision': 0.3264583333333317, 'Recall': 0.8234375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa4c588a0c24f45bf0872a98e1a5882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4646211568287549, 'Precision': 0.3245833333333318, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318fa182df734f7a989220c48346a2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46583409194355774, 'Precision': 0.32552083333333176, 'Recall': 0.81875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f97c0c144543858b4f38aa6781bde2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46608664772727126, 'Precision': 0.3255208333333318, 'Recall': 0.8203125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fffa9b72fa42e69c310f6132dccc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4666591032855311, 'Precision': 0.32583333333333175, 'Recall': 0.821875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10344943efa4ad7a4b280c7a897b4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4669106317411386, 'Precision': 0.32583333333333175, 'Recall': 0.8234375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec0737b97ec4a3fa65cf60d90e4c433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46608664772727126, 'Precision': 0.3255208333333318, 'Recall': 0.8203125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3688883527c64cbabe966b18217406f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46640688903026284, 'Precision': 0.32583333333333175, 'Recall': 0.8203125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6065789c0aa045519e7cf777a463112c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4643009102938477, 'Precision': 0.3242708333333318, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cee54a888184abdb4597e8920f4a797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46658969854940907, 'Precision': 0.3255208333333318, 'Recall': 0.8234375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fe9adedfc14455b934e7e548a2a478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.464621156828755, 'Precision': 0.32458333333333184, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5cd50588144d94ab151c328b375cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46544564080407336, 'Precision': 0.32489583333333183, 'Recall': 0.8203125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e1ec650f04409baffb38f796aa9b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46608664772727126, 'Precision': 0.3255208333333318, 'Recall': 0.8203125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cceaa3d8ba054b29bdbbe3d45e9a5e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4665896985494091, 'Precision': 0.32552083333333176, 'Recall': 0.8234375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2008d0aa8da44c7c8005ff418caffa09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46430091029384774, 'Precision': 0.32427083333333184, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c335533e7349cd861c6841c7af1da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46430091029384774, 'Precision': 0.32427083333333184, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55d838b557e4be9a98414b6edec027b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46430091029384774, 'Precision': 0.32427083333333184, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da13f03a7dcd402dbd2e26e37350cd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46111654394845697, 'Precision': 0.32239583333333194, 'Recall': 0.809375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8a70640aaf4bf6a1830adc05289d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46226263645536964, 'Precision': 0.32302083333333187, 'Recall': 0.8125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda8b54a6bbd42c69a755fac01be67da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46200940428072607, 'Precision': 0.32302083333333187, 'Recall': 0.8109375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8672eabd3d09472f9f2e86cf59753546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4616896765597707, 'Precision': 0.32270833333333193, 'Recall': 0.8109375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabcd63804504768a465421158c60cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46200940428072607, 'Precision': 0.32302083333333187, 'Recall': 0.8109375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55dcd48bcda4bfd9766e0b860a13ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46290226460071354, 'Precision': 0.3236458333333318, 'Recall': 0.8125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0185db97e14adaaab850141de78417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46150084054900364, 'Precision': 0.3230208333333318, 'Recall': 0.8078125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34d0a7294914697b1171058586e47a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4606079833871698, 'Precision': 0.32239583333333194, 'Recall': 0.80625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948e0ac49c8a41aca8b98d9714363760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.460288958641062, 'Precision': 0.32208333333333194, 'Recall': 0.80625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa11122db67b4b7e927569347c29d284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46111654394845686, 'Precision': 0.3223958333333319, 'Recall': 0.809375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a5f933d1f348438704aa42281a3371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4616896765597707, 'Precision': 0.32270833333333193, 'Recall': 0.8109375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936ae8c6465f4b63886674880aa20f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46372829067641536, 'Precision': 0.32395833333333185, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3389d5a72a4e798c4a51ac9df179f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46111654394845697, 'Precision': 0.32239583333333194, 'Recall': 0.809375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37033ce1219b44dab05b26f017e3c23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.45971512622526217, 'Precision': 0.321770833333332, 'Recall': 0.8046875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f8894a85fe484d9020ad9a7d35ed91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.45996975713362126, 'Precision': 0.32177083333333195, 'Recall': 0.80625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b257b8cf3bdf46f08b576c1ebf3f1948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46175547327752603, 'Precision': 0.323020833333332, 'Recall': 0.809375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902a8126b27d4491abdf93b68e2c917a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46054323776159173, 'Precision': 0.322083333333332, 'Recall': 0.8078125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893b0f7afc58438d940e9a7994043d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46290226460071365, 'Precision': 0.32364583333333186, 'Recall': 0.8125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146fcba8e7924b85a0ecd1286e7e3033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4649412281101772, 'Precision': 0.32489583333333183, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8dc35ba79541928b32b575e546d1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46436826237894974, 'Precision': 0.32458333333333184, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6469ce4e2fe4e37be9ffdae5eae5d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46340804151046755, 'Precision': 0.32364583333333186, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d1f8e3acec46aab92845b4af31fca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46200940428072607, 'Precision': 0.32302083333333187, 'Recall': 0.8109375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9e5c37b75a4768ad1676e320b98f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46258253851797365, 'Precision': 0.3233333333333319, 'Recall': 0.8125}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6332a4dafd8c460db55e99994d966562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46436826237894974, 'Precision': 0.32458333333333184, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c5d9c7307444009188a9fc2ff82ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4623936977107643, 'Precision': 0.32364583333333186, 'Recall': 0.809375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfc8726bc8e413a8d279537837c3883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4641146738633229, 'Precision': 0.3245833333333318, 'Recall': 0.8140625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96325d6274c4cf498906327e2c40a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4643682623789496, 'Precision': 0.3245833333333318, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5cc6484edc497bb75b0a109544bf7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4649412281101772, 'Precision': 0.3248958333333318, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537b254bb5df44f59743c393bda75d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4623289558269799, 'Precision': 0.32333333333333186, 'Recall': 0.8109375}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06018764e42e422b8a534f109bc3bb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4637951249084904, 'Precision': 0.3242708333333318, 'Recall': 0.8140625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e835f11627c640a1b82e93653742b3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46347540045766433, 'Precision': 0.32395833333333185, 'Recall': 0.8140625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457ac00fb61f4382a1c2ff6aba4946a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4643009102938477, 'Precision': 0.3242708333333318, 'Recall': 0.8171875}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bf5c6797f1439096aa451910ba7c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46404836425111784, 'Precision': 0.3242708333333318, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b0c546def24e2797b2e4be940d8238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46404836425111784, 'Precision': 0.3242708333333318, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76379e58b42742749e3534f67e48d497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.46404836425111784, 'Precision': 0.3242708333333318, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003dfe7b2d824501abf7746cb72ac010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4643682623789496, 'Precision': 0.3245833333333318, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bacd73e34640d0a4b5abad44a11f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4643682623789496, 'Precision': 0.3245833333333318, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf36da79ca44e2eb1fb1a788285a0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4643682623789496, 'Precision': 0.3245833333333318, 'Recall': 0.815625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3a63db3e10406da692dd3cff6da067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F1 score': 0.4643682623789496, 'Precision': 0.3245833333333318, 'Recall': 0.815625}\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "current_steps = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
    "\n",
    "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "        y_true = batch[\"labels\"].tolist()\n",
    "\n",
    "        current_steps += 1\n",
    "\n",
    "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
    "            print(\"Start validation\")\n",
    "            probs = get_predicted_probs(model, eval_dataloader, device)\n",
    "\n",
    "            val_results = evaluate_retrieval(\n",
    "                probs=probs,\n",
    "                df_evidences=dev_evidences,\n",
    "                ground_truths=DEV_GT,\n",
    "                top_n=TOP_N,\n",
    "            )\n",
    "            print(val_results)\n",
    "\n",
    "            # log each metric separately to TensorBoard\n",
    "            for metric_name, metric_value in val_results.items():\n",
    "                writer.add_scalar(\n",
    "                    f\"dev_{metric_name}\",\n",
    "                    metric_value,\n",
    "                    current_steps,\n",
    "                )\n",
    "\n",
    "            save_checkpoint(model, CKPT_DIR, current_steps)\n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5253440610b7ee10\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5253440610b7ee10\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation part (15 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start final evaluations and write prediction files.\n",
      "Start calculating training scores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4551ae3ff7744d7ca734d3b6d5553197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores => {'F1 score': 0.48269295870007345, 'Precision': 0.34375000000000583, 'Recall': 0.81015625}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec591f0478c467c964b6ec579f9f5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores => {'F1 score': 0.46915228650262186, 'Precision': 0.32802083333333176, 'Recall': 0.8234375}\n"
     ]
    }
   ],
   "source": [
    "ckpt_name = \"model.1700.pt\"  #@param {type:\"string\"}\n",
    "model = load_model(model, ckpt_name, CKPT_DIR)\n",
    "print(\"Start final evaluations and write prediction files.\")\n",
    "\n",
    "train_evidences = pair_with_wiki_sentences_eval(\n",
    "    mapping=mapping,\n",
    "    df=pd.DataFrame(TRAIN_GT),\n",
    ")\n",
    "train_set = SentRetrievalBERTDataset(train_evidences, tokenizer, max_length=MAX_SEQ_LEN)\n",
    "train_dataloader = DataLoader(train_set, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "print(\"Start calculating training scores\")\n",
    "probs = get_predicted_probs(model, train_dataloader, device)\n",
    "train_results = evaluate_retrieval(\n",
    "    probs=probs,\n",
    "    df_evidences=train_evidences,\n",
    "    ground_truths=TRAIN_GT,\n",
    "    top_n=TOP_N,\n",
    "#     save_name=\"train_doc5sent.jsonl\",\n",
    "#     save_name=\"train_doc5sent\"+str(TOP_N)+\".jsonl\",\n",
    "    save_name=f\"train_doc5sent{TOP_N}.jsonl\",\n",
    ")\n",
    "print(f\"Training scores => {train_results}\")\n",
    "\n",
    "print(\"Start validation\")\n",
    "probs = get_predicted_probs(model, eval_dataloader, device)\n",
    "val_results = evaluate_retrieval(\n",
    "    probs=probs,\n",
    "    df_evidences=dev_evidences,\n",
    "    ground_truths=DEV_GT,\n",
    "    top_n=TOP_N,\n",
    "#     save_name=\"dev_doc5sent.jsonl\",\n",
    "#     save_name=\"dev_doc5sent\"+str(TOP_N)+\".jsonl\",\n",
    "    save_name=f\"dev_doc5sent{TOP_N}.jsonl\",\n",
    ")\n",
    "\n",
    "print(f\"Validation scores => {val_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Check on our test data\n",
    "(5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "lVFusJqjmex-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting the test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37a806fe007412887ea4643435ca7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = load_json(\"data/test_doc5.jsonl\")\n",
    "\n",
    "test_evidences = pair_with_wiki_sentences_eval(\n",
    "    mapping,\n",
    "    pd.DataFrame(test_data),\n",
    "    is_testset=True,\n",
    ")\n",
    "test_set = SentRetrievalBERTDataset(test_evidences, tokenizer, max_length=MAX_SEQ_LEN)\n",
    "test_dataloader = DataLoader(test_set, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "print(\"Start predicting the test data\")\n",
    "probs = get_predicted_probs(model, test_dataloader, device)\n",
    "evaluate_retrieval(\n",
    "    probs=probs,\n",
    "    df_evidences=test_evidences,\n",
    "    ground_truths=test_data,\n",
    "    top_n=TOP_N,\n",
    "    cal_scores=False,\n",
    "    save_name=f\"test_doc5sent{TOP_N}.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzl8a5JteT7"
   },
   "source": [
    "notebook3\n",
    "## PART 3. Claim verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "tgA1vcUyzjlx"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from dataset import BERTDataset\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    "    set_lr_scheduler,\n",
    ")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "TRAIN_DATA = load_json(\"data/train_doc5sent5.jsonl\")\n",
    "DEV_DATA = load_json(\"data/dev_doc5sent5.jsonl\")\n",
    "\n",
    "TRAIN_PKL_FILE = Path(\"data/train_doc5sent5.pkl\")\n",
    "DEV_PKL_FILE = Path(\"data/dev_doc5sent5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_path = Path(f\"data/mapping_claim_verification.json\")\n",
    "if mapping_path.exists():\n",
    "    mapping = json.load( open( \"data/mapping_claim_verification.json\" ) )\n",
    "else:\n",
    "    wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "    mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "    json.dump( mapping, open( \"data/mapping_claim_verification.json\", 'w' ) )\n",
    "    del wiki_pages\n",
    "\n",
    "# OLD VERSION\n",
    "# wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "# mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n",
    "# del wiki_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AICUP dataset with top-k evidence sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AicupTopkEvidenceBERTDataset(BERTDataset):\n",
    "    \"\"\"AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        item = self.data.iloc[idx]\n",
    "        claim = item[\"claim\"]\n",
    "        evidence = item[\"evidence_list\"]\n",
    "\n",
    "        # In case there are less than topk evidence sentences\n",
    "        pad = [\"[PAD]\"] * (self.topk - len(evidence))\n",
    "        evidence += pad\n",
    "        concat_claim_evidence = \" [SEP] \".join([*claim, *evidence])\n",
    "\n",
    "        concat = self.tokenizer(\n",
    "            concat_claim_evidence,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        label = LABEL2ID[item[\"label\"]] if \"label\" in item else -1\n",
    "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
    "\n",
    "        if \"label\" in item:\n",
    "            concat_ten[\"labels\"] = torch.tensor(label)\n",
    "\n",
    "        return concat_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model: torch.nn.Module, dataloader: DataLoader, device):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            y_true.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return {\"val_loss\": loss / len(dataloader), \"val_acc\": acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    for batch in tqdm(test_dl,\n",
    "                      total=len(test_dl),\n",
    "                      leave=False,\n",
    "                      desc=\"Predicting\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        pred = model(**batch).logits\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        preds.extend(pred.tolist())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_topk_evidence(\n",
    "    df: pd.DataFrame,\n",
    "    mapping: dict,\n",
    "    mode: str = \"train\",\n",
    "    topk: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
    "\n",
    "    Note:\n",
    "        After extraction, the dataset will be like this:\n",
    "               id     label         claim                           evidence            evidence_list\n",
    "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
    "        ..    ...       ...            ...                                ...                     ...\n",
    "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
    "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
    "\n",
    "        [946 rows x 5 columns]\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with evidence.\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        topk (int, optional): The topk evidence. Defaults to 5.\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with topk evidence_list.\n",
    "            The `evidence_list` column will be: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
    "    if \"evidence\" in df.columns:\n",
    "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
    "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
    "            if not isinstance(x[0][0], list) else x)\n",
    "\n",
    "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
    "    if mode == \"eval\":\n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
    "            mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "            for evi_id, evi_idx in x  # for each evidence list\n",
    "        ][:topk] if isinstance(x, list) else [])\n",
    "        print(df[\"evidence_list\"][:5])\n",
    "    else:\n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"evidence\"].parallel_map(lambda x: [\n",
    "            \" \".join([  # join evidence\n",
    "                mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "                for _, _, evi_id, evi_idx in evi_list\n",
    "            ]) if isinstance(evi_list, list) else \"\"\n",
    "            for evi_list in x  # for each evidence list\n",
    "        ][:1] if isinstance(x, list) else [])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Setup training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "\n",
    "MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"}\n",
    "TRAIN_BATCH_SIZE = 8  #@param {type:\"integer\"}\n",
    "TEST_BATCH_SIZE = 8  #@param {type:\"integer\"}\n",
    "SEED = 42  #@param {type:\"integer\"}\n",
    "LR = 1e-6  #@param {type:\"number\"}\n",
    "NUM_EPOCHS = 10  #@param {type:\"integer\"}\n",
    "MAX_SEQ_LEN = 256  #@param {type:\"integer\"}\n",
    "EVIDENCE_TOPK = 5  #@param {type:\"integer\"}\n",
    "VALIDATION_STEP = 100  #@param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILENAME = \"submission.jsonl\"\n",
    "\n",
    "EXP_DIR = f\"claim_verification/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_top{EVIDENCE_TOPK}\"\n",
    "LOG_DIR = \"logs/\" + EXP_DIR\n",
    "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
    "\n",
    "if not Path(LOG_DIR).exists():\n",
    "    Path(LOG_DIR).mkdir(parents=True)\n",
    "\n",
    "if not Path(CKPT_DIR).exists():\n",
    "    Path(CKPT_DIR).mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Concat claim and evidences\n",
    "join topk \n",
    "evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_PKL_FILE.exists():\n",
    "    train_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TRAIN_DATA),\n",
    "        mapping,\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    train_df.to_pickle(TRAIN_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(TRAIN_PKL_FILE, \"rb\") as f:\n",
    "        train_df = pickle.load(f)\n",
    "\n",
    "if not DEV_PKL_FILE.exists():\n",
    "    dev_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(DEV_DATA),\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    dev_df.to_pickle(DEV_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(DEV_PKL_FILE, \"rb\") as f:\n",
    "        dev_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevent CUDA out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "O0rVk3990DlD"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "val_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    ")\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "CzMgs-Zs3sTN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(LABEL2ID))\n",
    "config.hidden_dropout_prob = 0.2\n",
    "config.attention_probs_dropout_prob = 0.2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL2ID))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
    "\n",
    "writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (30 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "_aqMjEek3wmu"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77548ee1d82492f881ab539b2fdbc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e71b0ab4924cc7b4f68374c4f8b71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.1724776935577392\n",
      "val_acc: 0.20025188916876574\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bd082b41b64fc8acee685e1d3dd63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.064797071814537\n",
      "val_acc: 0.40428211586901763\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7630a6e440408fb3693df16ada1d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.046289986371994\n",
      "val_acc: 0.40554156171284633\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efcd52e95454f4fa6a9c0d3aa3c1680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.0956793999671937\n",
      "val_acc: 0.40554156171284633\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3cd1a2ccc4468caadfc3b3540a266a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.1900182938575745\n",
      "val_acc: 0.40428211586901763\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8bb163886547289d929e9122dca8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.2913585525751115\n",
      "val_acc: 0.4093198992443325\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba3e13dc8f6484ab14d0e8b4209c239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.3349124109745025\n",
      "val_acc: 0.4105793450881612\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826a5d0871d2431ba415907d59cd4efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.3362459594011307\n",
      "val_acc: 0.41183879093198994\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b30992fcd64d48a39bdd7a105992eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.3941460812091828\n",
      "val_acc: 0.41183879093198994\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7d1bcdb7a84e1da8ef716e3205a133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.4036365890502929\n",
      "val_acc: 0.4080604534005038\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba5f6078e7d4c269f2162c01549bd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.410123153924942\n",
      "val_acc: 0.4093198992443325\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e77ddb6dce44849e9b1aceed88b077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.4645028388500214\n",
      "val_acc: 0.4093198992443325\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1636e086ef44c9b86aa8682ba96f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.4789185154438018\n",
      "val_acc: 0.41435768261964734\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28e2cbdfb914ebba591b13c4855e6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.4765325695276261\n",
      "val_acc: 0.42065491183879095\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ec0f6a9eb449da881ce6898d2dea17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.493186427950859\n",
      "val_acc: 0.4168765743073048\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d06ef643464cde980d0e3214730e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.4931908667087554\n",
      "val_acc: 0.4105793450881612\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30f554d87e34f499e3ca180f1a79fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.546320687532425\n",
      "val_acc: 0.4105793450881612\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96700a95360044a7a09760a75e3f4fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.5661044442653655\n",
      "val_acc: 0.4093198992443325\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657fac8690294709892f9e9465d9b650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.570606762766838\n",
      "val_acc: 0.4068010075566751\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519f97b2747b4bea9c8ef8bf5a294482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.5527435302734376\n",
      "val_acc: 0.41435768261964734\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5848bf94170f4b648d1cda0e22968ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.5805345606803893\n",
      "val_acc: 0.4093198992443325\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f2d22d5dc946689a391d65891ee0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.5866148585081101\n",
      "val_acc: 0.4080604534005038\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b009f4c54e4e56955171795bf199ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.5903266471624375\n",
      "val_acc: 0.4093198992443325\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004d4169fe874e6eb3ed4679850e16ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.5775149899721146\n",
      "val_acc: 0.4080604534005038\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e0eed763f446379177aebca45e6c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.618773390650749\n",
      "val_acc: 0.4080604534005038\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167867cfabea478a936167fdec098c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.5940221613645553\n",
      "val_acc: 0.4080604534005038\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d97d176f668426597d9ef08b3092508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.6452614277601243\n",
      "val_acc: 0.4093198992443325\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109302bc5fdf4b1993fe9bc6d00f2051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.632832238972187\n",
      "val_acc: 0.4105793450881612\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8c548b06dc423d8dbc45c5e3069876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.6393212151527405\n",
      "val_acc: 0.4105793450881612\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913ea2d7638f4af18b39a91c11b3d221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.6515728795528413\n",
      "val_acc: 0.4080604534005038\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5835abcd4f4b4a6793676c768c9ceb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.649721218943596\n",
      "val_acc: 0.4093198992443325\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76485a9ec3734df2ab1f078e60d637c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.6572446432709693\n",
      "val_acc: 0.4068010075566751\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50ffe9595ee40eb8ef7888126bc1e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.6624558275938035\n",
      "val_acc: 0.4068010075566751\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af386fe197449779721a3d7c80a7e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.6715858888626098\n",
      "val_acc: 0.4068010075566751\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd5c3953cd4402da98ae3bb5c8dcf2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.6756228053569793\n",
      "val_acc: 0.4080604534005038\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd3662082684e10a34c7fa43c4e389d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.6788299867510796\n",
      "val_acc: 0.4068010075566751\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4abb59a7c44f769684930cce4a880a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.6795055854320526\n",
      "val_acc: 0.4068010075566751\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e7116689384cbe9fba34f5cdaa0531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.6832346433401109\n",
      "val_acc: 0.4080604534005038\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca182fd29974f61b35c62b1ffa6ace6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.684688812494278\n",
      "val_acc: 0.4068010075566751\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "current_steps = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
    "\n",
    "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "        y_true = batch[\"labels\"].tolist()\n",
    "\n",
    "        current_steps += 1\n",
    "\n",
    "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
    "            print(\"Start validation\")\n",
    "            val_results = run_evaluation(model, eval_dataloader, device)\n",
    "\n",
    "            # log each metric separately to TensorBoard\n",
    "            for metric_name, metric_value in val_results.items():\n",
    "                print(f\"{metric_name}: {metric_value}\")\n",
    "                writer.add_scalar(f\"{metric_name}\", metric_value, current_steps)\n",
    "\n",
    "            save_checkpoint(\n",
    "                model,\n",
    "                CKPT_DIR,\n",
    "                current_steps,\n",
    "                mark=f\"val_acc={val_results['val_acc']:.4f}\",\n",
    "            )\n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Make your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "zLkfuoAE49mz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting evidence_list for the eval mode ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507a2a3505b048368675535e67494d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2260), Label(value='0 / 2260'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [顯微鏡泛指將微小不可見或難見物品之影像放大 ， 而能被肉眼或其他成像儀器觀察之工具 。, ...\n",
      "1    [許多昆蟲被認爲是對生態有益的捕食者 ， 少數昆蟲提供直接的經濟利益 。, 蠶產絲 ， 蜜蜂...\n",
      "2    [綠山城縣  ， 是波蘭的縣份 ， 位於該國西部 ， 由盧布斯卡省負責管轄 ， 首府設於綠山...\n",
      "3    [《 魂斷藍橋 》 （ Waterloo Bridge ） 是美國黑白電影 ， 由米高梅電影...\n",
      "4    [2015年以 《 刺客聶隱娘 》 獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎...\n",
      "Name: evidence_list, dtype: object\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA = load_json(\"data/test_doc5sent5.jsonl\")\n",
    "TEST_PKL_FILE = Path(\"data/test_doc5sent5.pkl\")\n",
    "\n",
    "if not TEST_PKL_FILE.exists():\n",
    "    test_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TEST_DATA),\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    test_df.to_pickle(TEST_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(TEST_PKL_FILE, \"rb\") as f:\n",
    "        test_df = pickle.load(f)\n",
    "\n",
    "test_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "tqIjlht8yCMA"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt_name = \"val_acc=0.4207_model.1400.pt\"  #@param {type:\"string\"}\n",
    "\n",
    "model = load_model(model, ckpt_name, CKPT_DIR)\n",
    "predicted_label = run_predict(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "Gl9I3ZWW4pHo"
   },
   "outputs": [],
   "source": [
    "predict_dataset = test_df.copy()\n",
    "predict_dataset[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label))\n",
    "predict_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
    "    OUTPUT_FILENAME,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "10286f3c74912972f7d1fdceceee5be5b7c77248e5efe5afcbc6a71f24d230fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
